{"cells":[{"cell_type":"code","execution_count":1,"id":"33b675fd","metadata":{"id":"33b675fd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647234930675,"user_tz":420,"elapsed":10678,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}},"outputId":"f0eb17a5-3a19-4b87-f5d3-e808a026b875"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scanpy in /usr/local/lib/python3.7/dist-packages (1.8.2)\n","Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.0.2)\n","Requirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.51.2)\n","Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.4.1)\n","Requirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (from scanpy) (5.5.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from scanpy) (4.63.0)\n","Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.7/dist-packages (from scanpy) (2.6.3)\n","Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.1.0)\n","Requirement already satisfied: pandas>=0.21 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.3.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scanpy) (21.3)\n","Requirement already satisfied: umap-learn>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.5.2)\n","Requirement already satisfied: anndata>=0.7.4 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.7.8)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.21.5)\n","Requirement already satisfied: sinfo in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.3.4)\n","Requirement already satisfied: statsmodels>=0.10.0rc2 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.10.2)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.11.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.1.0)\n","Requirement already satisfied: patsy in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.5.2)\n","Requirement already satisfied: importlib_metadata>=0.7 in /usr/local/lib/python3.7/dist-packages (from scanpy) (4.11.2)\n","Requirement already satisfied: matplotlib>=3.1.2 in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.2.2)\n","Requirement already satisfied: tables in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.7.0)\n","Requirement already satisfied: xlrd<2.0 in /usr/local/lib/python3.7/dist-packages (from anndata>=0.7.4->scanpy) (1.1.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.10.0->scanpy) (1.5.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.7->scanpy) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.7->scanpy) (3.7.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (3.0.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (1.3.2)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (0.34.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (57.4.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21->scanpy) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy) (1.15.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->scanpy) (3.1.0)\n","Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.3.10->scanpy) (0.5.6)\n","Requirement already satisfied: stdlib-list in /usr/local/lib/python3.7/dist-packages (from sinfo->scanpy) (0.8.0)\n","Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.7/dist-packages (from tables->scanpy) (2.8.1)\n"]}],"source":["!pip install scanpy\n","import os\n","import numpy as np\n","import pandas as pd\n","import scanpy as sc\n","import time\n","from scipy.stats import pearsonr\n","#\n","#more 10x datasets \n","#https://support.10xgenomics.com/single-cell-multiome-atac-gex/datasets/\n","\n","\n","#switch to keras for custom ae\n","#https://blog.keras.io/building-autoencoders-in-keras.html\n","#merging nerual networks\n","#https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras import Model\n","from tensorflow.keras import backend as K\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","#make a shortcut to emily's shared drive folder in your drive so you can access the data at\n","import os\n","os.listdir('/content/drive/My Drive/methyl_impute')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FmhHyvpd9Qj1","executionInfo":{"status":"ok","timestamp":1647234939174,"user_tz":420,"elapsed":1660,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}},"outputId":"a0bf25a1-cb8d-4ffd-e0e1-f81ddef8fd35"},"id":"FmhHyvpd9Qj1","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["['Peek_data.ipynb',\n"," 'Project Proposal Brainstorming.gdoc',\n"," 'adult_cortex_CG-CH_OLDCOPY.h5ad',\n"," 'adult_cortex_CG_CH.h5ad',\n"," 'tf_multiclass_prediction.ipynb',\n"," 'classifier_results.gslides',\n"," 'ch_isImputed.parq',\n"," 'cg_isImputed.parq',\n"," 'PredictionModel.ipynb',\n"," 'vanilla tuning',\n"," 'VAE_impute (variational+loss).ipynb',\n"," 'final_imputation_mask.parq',\n"," 'VAE_impute (vanilla) (custom dropout+loss).ipynb',\n"," 'VAE_tune',\n"," 'VAE_impute (vanilla+tuned).ipynb',\n"," 'VAE_impute (vanilla) (custom dropout).ipynb',\n"," 'Presentation.gslides',\n"," 'VAE_impute (variational).ipynb',\n"," 'Project_Report.gdoc',\n"," 'VAE_impute (variational+tuned).ipynb',\n"," 'final_imputation_mask_2.parq',\n"," 'final_imputation_mask_2.csv',\n"," 'VAE_impute (variational) (custom dropout+custom loss).ipynb',\n"," 'test_predictions_top_genes.pickle',\n"," 'results.gdoc',\n"," 'test_predictions_top_genes_plus_25.pickle',\n"," 'test_predictions_top_genes_plus_50.pickle',\n"," 'test_predictions_top_genes_plus_75.pickle']"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","execution_count":3,"id":"83451993","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"83451993","executionInfo":{"status":"ok","timestamp":1647234949745,"user_tz":420,"elapsed":4547,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}},"outputId":"a46e3c70-994e-45bf-a08b-7b139b62bb3a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["AnnData object with n_obs × n_vars = 11945 × 44772\n","    obs: 'sample', 'L1', 'L2', 'L3', 'true_batch', 'age', 'age_groups', 'leiden'\n","    var: 'batch'\n","    uns: 'L2_colors', 'L3_colors', 'leiden', 'neighbors', 'pca', 'umap'\n","    obsm: 'X_pca', 'X_umap'\n","    varm: 'PCs'\n","    obsp: 'connectivities', 'distances'"]},"metadata":{},"execution_count":3}],"source":["adata=sc.read_h5ad('/content/drive/My Drive/methyl_impute/adult_cortex_CG_CH.h5ad')\n","adata"]},{"cell_type":"code","execution_count":4,"id":"f04f44c7","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"id":"f04f44c7","outputId":"ebd44394-6ae5-4f7a-f637-25cb1ebd0a64","executionInfo":{"status":"ok","timestamp":1647234037643,"user_tz":420,"elapsed":50563,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING: Default of the method has been changed to 't-test' from 't-test_overestim_var'\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n","/usr/local/lib/python3.7/dist-packages/scanpy/tools/_rank_genes_groups.py:394: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n","  self.stats[group_name, 'names'] = self.var_names[global_indices]\n","/usr/local/lib/python3.7/dist-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n","  self.stats[group_name, 'scores'] = scores[global_indices]\n","/usr/local/lib/python3.7/dist-packages/scanpy/tools/_rank_genes_groups.py:399: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n","  self.stats[group_name, 'pvals'] = pvals[global_indices]\n","/usr/local/lib/python3.7/dist-packages/scanpy/tools/_rank_genes_groups.py:409: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n","  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n","/usr/local/lib/python3.7/dist-packages/scanpy/tools/_rank_genes_groups.py:421: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n","  foldchanges[global_indices]\n"]},{"output_type":"execute_result","data":{"text/plain":["\"adata2=adata[:,markers]\\nsc.pp.scale(adata2)\\nsc.pl.heatmap(adata2, markers, groupby='L3',vmax=1,vmin=-1)\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"source":["#collect top marker genes\n","\n","sc.tl.rank_genes_groups(adata, 'L3',n_genes=20)\n","# sc.pl.rank_genes_groups(adata, sharey=False)\n","markers=[]\n","for i in range(27):\n","    for j in range(20):\n","        if adata.uns['rank_genes_groups']['names'][j][i] not in markers:\n","            markers.append(adata.uns['rank_genes_groups']['names'][j][i])\n","\"\"\"adata2=adata[:,markers]\n","sc.pp.scale(adata2)\n","sc.pl.heatmap(adata2, markers, groupby='L3',vmax=1,vmin=-1)\"\"\""]},{"cell_type":"code","source":["import random\n","\n","print(random.random())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T_ylLOKglXkB","executionInfo":{"status":"ok","timestamp":1647234052003,"user_tz":420,"elapsed":498,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}},"outputId":"19470f71-fe75-4e63-e3be-84b7c9b067b6"},"id":"T_ylLOKglXkB","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["0.1374989565502055\n"]}]},{"cell_type":"code","source":["for item in adata.var.index.values:\n","    if item not in markers:\n","        if random.random() <= 0.75:\n","            markers.append(item)\n","\n","print(len(markers))        "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jOCNVm9mlEIs","executionInfo":{"status":"ok","timestamp":1647234070095,"user_tz":420,"elapsed":11782,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}},"outputId":"65870db2-a335-46cb-b206-3e7cc7f717e2"},"id":"jOCNVm9mlEIs","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["33793\n"]}]},{"cell_type":"code","source":["markers = adata.var.index.values\n","print(len(markers))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fwTKlGDG3ILP","executionInfo":{"status":"ok","timestamp":1647235133812,"user_tz":420,"elapsed":511,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}},"outputId":"8b955797-01c8-4dd8-c847-dd86f4f7aed5"},"id":"fwTKlGDG3ILP","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["44772\n"]}]},{"cell_type":"code","execution_count":8,"id":"ad7d7fb3","metadata":{"id":"ad7d7fb3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647234074500,"user_tz":420,"elapsed":311,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}},"outputId":"39b8776f-dc0f-4367-e642-709593435ca2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["View of AnnData object with n_obs × n_vars = 11945 × 33793\n","    obs: 'sample', 'L1', 'L2', 'L3', 'true_batch', 'age', 'age_groups', 'leiden'\n","    var: 'batch'\n","    uns: 'L2_colors', 'L3_colors', 'leiden', 'neighbors', 'pca', 'umap', 'rank_genes_groups'\n","    obsm: 'X_pca', 'X_umap'\n","    varm: 'PCs'\n","    obsp: 'connectivities', 'distances'"]},"metadata":{},"execution_count":8}],"source":["adata=adata[:,markers]\n","adata"]},{"cell_type":"code","execution_count":4,"id":"114f92a6","metadata":{"id":"114f92a6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647234973252,"user_tz":420,"elapsed":820,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}},"outputId":"3af23e73-b998-4011-945e-abc86385c8a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["(11945,)\n"]}],"source":["#data matrix, X\n","\"\"\"X=np.array(adata.X)\n","print(X.shape)\"\"\"\n","\n","yl=adata.obs['L3']\n","print(yl.shape)"]},{"cell_type":"code","source":["final_mask = pd.read_parquet('/content/drive/My Drive/methyl_impute/final_imputation_mask.parq')"],"metadata":{"id":"Zr-xs2WxSxWT","executionInfo":{"status":"ok","timestamp":1647234982147,"user_tz":420,"elapsed":6234,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}}},"id":"Zr-xs2WxSxWT","execution_count":5,"outputs":[]},{"cell_type":"code","source":["final_mask = final_mask[adata.to_df().columns.values].values"],"metadata":{"id":"zFs0PwbQSz3-","executionInfo":{"status":"ok","timestamp":1647234103388,"user_tz":420,"elapsed":5654,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}}},"id":"zFs0PwbQSz3-","execution_count":11,"outputs":[]},{"cell_type":"code","source":["final_mask.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Z4VwY4wmHRs","executionInfo":{"status":"ok","timestamp":1647234984824,"user_tz":420,"elapsed":307,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}},"outputId":"3d1cdb72-0c6c-4ed8-a74e-667b18db36d7"},"id":"7Z4VwY4wmHRs","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(11945, 44772)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["class DropoutWithInference(layers.Layer):\n","    def __init__(self, rate, **kwargs):\n","        super(DropoutWithInference, self).__init__(**kwargs)\n","        self.rate = rate\n","\n","    def _mask(self, args):\n","        methyl, boolean_imput = args\n","        methyl *= boolean_imput\n","        return methyl\n","\n","    def call(self, inputs, training=None):\n","        methyl, boolean_impute = inputs\n","        if training:\n","            return layers.Dropout(rate=self.rate)(methyl)\n","        else:\n","            return layers.Lambda(self._mask)(inputs)"],"metadata":{"id":"TiaQ2qrKS-HU","executionInfo":{"status":"ok","timestamp":1647234987983,"user_tz":420,"elapsed":293,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}}},"id":"TiaQ2qrKS-HU","execution_count":7,"outputs":[]},{"cell_type":"code","source":["class VAE:\n","  def __init__(self, input_shape,batch_size=256,optimizer=keras.optimizers.Adam(learning_rate=.001),epochs=100,\n","               recon_loss_function=tf.keras.losses.MeanSquaredError(),callback=tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)):\n","    self.mu = None\n","    self.sigma = None\n","\n","    self.input_shape=input_shape\n","    self.batch_size=batch_size\n","    self.n_epochs=epochs\n","    self.optimizer=optimizer\n","    self.recon_loss_function=recon_loss_function\n","    self.callback=callback\n","    self.encoder=self.generate_encoder()\n","    self.encoder.compile(optimizer=optimizer)\n","    self.sampler=self.perform_sampling()\n","    self.decoder=self.generate_decoder()\n","    self.decoder.compile(optimizer=optimizer)\n","\n","  def vae_loss(self, y_true, y_pred, mask):\n","    recon = self.custom_recon_loss(y_true, y_pred, mask)\n","    kl = K.mean(0.5*K.sum(K.exp(self.sigma) + K.square(self.mu) - 1. - self.sigma, axis=1))\n","    kl = tf.cast(kl, tf.float64)\n","    return recon + kl/200\n","\n","  def sample_z(self, args):\n","    vae_mu, vae_sigma = args\n","    eps = K.random_normal(shape=(K.shape(vae_mu)[0], K.shape(vae_mu)[1]), mean=0., stddev=1., seed=42)\n","    return vae_mu + K.exp(vae_sigma/2)*eps\n","\n","  def kl_loss(self):\n","    return K.mean(0.5*K.sum(K.exp(self.sigma) + K.square(self.mu) - 1. - self.sigma, axis=1))\n","\n","  def custom_recon_loss(self, label_true, prediction, mask):\n","    squared_error = tf.math.square(tf.math.subtract(tf.cast(label_true, tf.float64), tf.cast(prediction, tf.float64)))\n","    masked_error = tf.math.multiply(squared_error, mask)\n","    return tf.math.reduce_mean(masked_error)\n","\n","  def generate_encoder(self):\n","    mC_input=keras.Input(shape=(self.input_shape,),name='mC_input')\n","    imputed_boolean_input = keras.Input(shape=(self.input_shape,), name='imput_mask_input')\n","    input_dropout = DropoutWithInference(0.25, name='custom_dropout')([mC_input, imputed_boolean_input])\n","    dense1=layers.Dense(128,activation='relu', name='dense1')(input_dropout)\n","    # dropout1=layers.Dropout(0.25, name='dropout1')(dense1)\n","    dense2=layers.Dense(64,activation='relu', name='dense2')(dense1)\n","    # dropout2=layers.Dropout(0.25, name='dropout2')(dense2)\n","    vae_mu=layers.Dense(16,activation='linear')(dense2)\n","    vae_sigma=layers.Dense(16,activation='linear')(dense2)\n","\n","    return Model([mC_input, imputed_boolean_input], (vae_mu, vae_sigma), name='encoder')\n","\n","  def perform_sampling(self):\n","    vae_mu = keras.Input(shape=(16,))\n","    vae_sigma = keras.Input(shape=(16,))\n","    out=layers.Lambda(self.sample_z)([vae_mu, vae_sigma])\n","    return Model([vae_mu, vae_sigma], out, name='sampler')\n","\n","  def generate_decoder(self):\n","    input_latent = keras.Input(shape=(16,))\n","    out_pred=layers.Dense(self.input_shape,activation='linear',name='out_pred')(input_latent)\n","    return Model(input_latent, out_pred, name='decoder')\n","\n","  def train_vae(self,train_dataset_vals,train_dataset_mask,test_dataset_vals,test_dataset_mask):\n","    epochs = self.n_epochs\n","    for epoch in range(epochs):\n","        print(\"\\nStart of epoch %d\" % (epoch,))\n","        epoch_train_losses = []\n","\n","        # Iterate over the batches of the dataset.\n","        for (step, (x_batch_train_vals)), (step, (x_batch_train_mask)) in zip(enumerate(train_dataset_vals), enumerate(train_dataset_mask)):\n","            with tf.GradientTape() as encoder_tape, tf.GradientTape() as decoder_tape:\n","                self.mu, self.sigma = self.encoder([x_batch_train_vals, x_batch_train_mask], training=True)\n","                latent = self.sampler([self.mu, self.sigma])\n","                logits=self.decoder(latent, training=True)\n","                loss_value = self.vae_loss(x_batch_train_vals, logits, x_batch_train_mask)\n","\n","            encoder_grads = encoder_tape.gradient(loss_value, self.encoder.trainable_weights)\n","            decoder_grads = decoder_tape.gradient(loss_value, self.decoder.trainable_weights)\n","            self.optimizer.apply_gradients(zip(encoder_grads, self.encoder.trainable_weights))\n","            self.optimizer.apply_gradients(zip(decoder_grads, self.decoder.trainable_weights))\n","\n","            # Update training metric.\n","            epoch_train_losses.append(self.vae_loss(x_batch_train_vals, logits, x_batch_train_mask))\n","\n","            # Log every 31 batches.\n","            if step % 31 == 0:\n","                print(\n","                    \"Training loss (for one batch) at step %d: %.4f\"\n","                    % (step, float(loss_value))\n","                )\n","\n","        # Display metrics at the end of each epoch.\n","        train_acc = sum(epoch_train_losses)/len(epoch_train_losses)\n","        print(\"Reconstruction Training acc over epoch: %.4f\" % (float(train_acc),))\n","        print(\"KL Divergence: %.10f\" % (float(self.kl_loss())))\n","\n","        epoch_val_losses = []\n","        # Run a validation loop at the end of each epoch.\n","        for (x_batch_test_vals, x_batch_test_mask) in zip(test_dataset_vals, test_dataset_mask):\n","            self.mu, self.sigma = self.encoder([x_batch_test_vals, x_batch_test_mask], training=False)\n","            latent = self.sampler([self.mu, self.sigma])\n","            test_logits = self.decoder(latent, training=False)\n","            epoch_val_losses.append(self.vae_loss(x_batch_test_vals, test_logits, x_batch_test_mask))\n","        print(\"Validation loss: %.4f\" % (float(sum(epoch_val_losses)/len(epoch_val_losses)),))\n","\n","  def predict(self,X_vals,X_mask):\n","    self.mu, self.sigma = self.encoder.predict([X_vals, X_mask])\n","    latent = self.sampler([self.mu, self.sigma])\n","    return self.decoder.predict(latent)\n","\n","  def model_summary(self):\n","    print(self.model.summary())\n","\n","  def visualize_model(self):\n","    #dunno how to make this show from the function lol\n","    keras.utils.plot_model(self.model, show_shapes=True, show_layer_names=True)\n","\n","  def correlation_accuracy(self, real_data_vals, real_data_mask):\n","    reconstructed_data = self.predict(real_data_vals, real_data_mask)\n","    corrs = 0\n","    for i in range(len(reconstructed_data)):\n","        corrs += pearsonr(reconstructed_data[i], real_data_vals[i])[0]\n","\n","    print(corrs/len(reconstructed_data))\n","\n","  def mse_error(self, real_data_vals, real_data_mask):\n","    reconstructed_data = self.predict(real_data_vals, real_data_mask)\n","    mse = 0\n","    for i in range(len(reconstructed_data)):\n","        mse += mean_squared_error(reconstructed_data[i], real_data_vals[i])\n","\n","    print(self.custom_recon_loss(real_data_vals, reconstructed_data, real_data_mask))\n","    print(mse/len(reconstructed_data))"],"metadata":{"id":"hv7PmI4s6TpB","executionInfo":{"status":"ok","timestamp":1647234990424,"user_tz":420,"elapsed":2,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}}},"id":"hv7PmI4s6TpB","execution_count":8,"outputs":[]},{"cell_type":"code","source":["temp_input = np.concatenate([adata.X, final_mask], axis=1)\n","\n","X_train, X_test, y_train, y_test = train_test_split(temp_input, yl, test_size=0.33, random_state=42,shuffle=True)\n","X_train_vals = X_train[:, :adata.X.shape[1]]\n","X_train_mask = X_train[:, adata.X.shape[1]:]\n","X_test_vals = X_test[:, :adata.X.shape[1]]\n","X_test_mask = X_test[:, adata.X.shape[1]:]\n","print(X_train_vals.shape)\n","print(X_test_vals.shape)\n","print(X_train_mask.shape)\n","print(X_test_mask.shape)\n"],"metadata":{"id":"z3oSDp4EWQFb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647235036751,"user_tz":420,"elapsed":23361,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}},"outputId":"67446877-26c9-4056-b29d-4d3ff22a2f09"},"id":"z3oSDp4EWQFb","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["(8003, 44772)\n","(3942, 44772)\n","(8003, 44772)\n","(3942, 44772)\n"]}]},{"cell_type":"code","source":["print(y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j9twzZUcf-Wl","executionInfo":{"status":"ok","timestamp":1647235041655,"user_tz":420,"elapsed":818,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}},"outputId":"a4fe3ed6-7442-4f6e-ec6c-482673922893"},"id":"j9twzZUcf-Wl","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["20210528-UMB5577-preAb-PFC-B8_O8        MGE ERBB4\n","20210224-5577-postAb-PFC-C08_F10              ODC\n","20210505-UMB6096-preAb-PFC-B03_F3       L1-3 CUX2\n","20210528-UMB5621-preAb-PFC-A1_A24             ODC\n","SRR9012214                                L4 RORB\n","                                         ...     \n","20210528-UMB5621-preAb-PFC-A2_O24           Astro\n","SRR9012607                                  Astro\n","SRR9012169                                  Astro\n","20210505-UMB5577-preAb-PFC-B09_F16    CT-L6 ASTN1\n","20210505-UMB5577-preAb-PFC-A09_O5      CGE SPOCK3\n","Name: L3, Length: 3942, dtype: category\n","Categories (27, object): ['Astro', 'CGE SOX13', 'CGE SPOCK3', 'CGE TOX', ..., 'ODC', 'OPC', 'PC',\n","                          'VLMC']\n"]}]},{"cell_type":"code","source":["temp_input = None"],"metadata":{"id":"4fsX_k6e3Rxe","executionInfo":{"status":"ok","timestamp":1647235047372,"user_tz":420,"elapsed":300,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}}},"id":"4fsX_k6e3Rxe","execution_count":11,"outputs":[]},{"cell_type":"code","source":["final_mask = None"],"metadata":{"id":"AdFYqHr53XRC","executionInfo":{"status":"ok","timestamp":1647235048566,"user_tz":420,"elapsed":2,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}}},"id":"AdFYqHr53XRC","execution_count":12,"outputs":[]},{"cell_type":"code","source":["adata = None"],"metadata":{"id":"sI7QTk8KcSoP","executionInfo":{"status":"ok","timestamp":1647235143131,"user_tz":420,"elapsed":377,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}}},"id":"sI7QTk8KcSoP","execution_count":17,"outputs":[]},{"cell_type":"code","source":["X_train = None"],"metadata":{"id":"pJK4uboIcW_s","executionInfo":{"status":"ok","timestamp":1647235053973,"user_tz":420,"elapsed":305,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}}},"id":"pJK4uboIcW_s","execution_count":13,"outputs":[]},{"cell_type":"code","source":["X_test = None"],"metadata":{"id":"8wuNftsgcY3g","executionInfo":{"status":"ok","timestamp":1647235055486,"user_tz":420,"elapsed":2,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}}},"id":"8wuNftsgcY3g","execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":18,"id":"408d4bc6","metadata":{"id":"408d4bc6","executionInfo":{"status":"ok","timestamp":1647235149760,"user_tz":420,"elapsed":3253,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}}},"outputs":[],"source":["vae=VAE(X_train_vals.shape[1])"]},{"cell_type":"code","execution_count":19,"id":"00d6a051","metadata":{"id":"00d6a051","executionInfo":{"status":"ok","timestamp":1647235160670,"user_tz":420,"elapsed":8189,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}}},"outputs":[],"source":["batch_size=256\n","\n","train_dataset_vals=tf.data.Dataset.from_tensor_slices(X_train_vals)\n","train_dataset_vals = train_dataset_vals.shuffle(buffer_size=1024, seed=42).batch(batch_size)\n","\n","train_dataset_mask=tf.data.Dataset.from_tensor_slices(X_train_mask)\n","train_dataset_mask = train_dataset_mask.shuffle(buffer_size=1024, seed=42).batch(batch_size)\n","\n","test_dataset_vals = tf.data.Dataset.from_tensor_slices(X_test_vals)\n","test_dataset_vals = test_dataset_vals.batch(batch_size)\n","\n","test_dataset_mask = tf.data.Dataset.from_tensor_slices(X_test_mask)\n","test_dataset_mask = test_dataset_mask.batch(batch_size)"]},{"cell_type":"code","source":["train_dataset_vals = None\n","train_dataset_mask = None\n","test_dataset_vals = None\n","test_dataset_mask = None"],"metadata":{"id":"W4nP0iYU68Nz","executionInfo":{"status":"ok","timestamp":1647236102846,"user_tz":420,"elapsed":309,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}}},"id":"W4nP0iYU68Nz","execution_count":30,"outputs":[]},{"cell_type":"code","source":["X_train_vals = None\n","X_train_mask = None\n","# X_test_vals = None\n","# X_test_mask = None"],"metadata":{"id":"aGnwW86d3sP6","executionInfo":{"status":"ok","timestamp":1647235163551,"user_tz":420,"elapsed":307,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}}},"id":"aGnwW86d3sP6","execution_count":20,"outputs":[]},{"cell_type":"code","source":["vae.train_vae(train_dataset_vals,train_dataset_mask,test_dataset_vals,test_dataset_mask)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sMoN2Z8zPQl2","executionInfo":{"status":"ok","timestamp":1647235904897,"user_tz":420,"elapsed":731046,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}},"outputId":"b9618214-3f4f-47f9-d8a0-a8c991d6eb26"},"id":"sMoN2Z8zPQl2","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Start of epoch 0\n","Training loss (for one batch) at step 0: 0.1841\n","Training loss (for one batch) at step 31: 0.1363\n","Reconstruction Training acc over epoch: 0.3440\n","KL Divergence: 5.5703349113\n","Validation loss: 0.1363\n","\n","Start of epoch 1\n","Training loss (for one batch) at step 0: 0.1356\n","Training loss (for one batch) at step 31: 0.0795\n","Reconstruction Training acc over epoch: 0.1039\n","KL Divergence: 6.7262020111\n","Validation loss: 0.0746\n","\n","Start of epoch 2\n","Training loss (for one batch) at step 0: 0.0748\n","Training loss (for one batch) at step 31: 0.0503\n","Reconstruction Training acc over epoch: 0.0603\n","KL Divergence: 5.0526971817\n","Validation loss: 0.0500\n","\n","Start of epoch 3\n","Training loss (for one batch) at step 0: 0.0490\n","Training loss (for one batch) at step 31: 0.0446\n","Reconstruction Training acc over epoch: 0.0454\n","KL Divergence: 4.6226949692\n","Validation loss: 0.0417\n","\n","Start of epoch 4\n","Training loss (for one batch) at step 0: 0.0410\n","Training loss (for one batch) at step 31: 0.0378\n","Reconstruction Training acc over epoch: 0.0396\n","KL Divergence: 4.1276488304\n","Validation loss: 0.0378\n","\n","Start of epoch 5\n","Training loss (for one batch) at step 0: 0.0371\n","Training loss (for one batch) at step 31: 0.0360\n","Reconstruction Training acc over epoch: 0.0373\n","KL Divergence: 3.2004678249\n","Validation loss: 0.0359\n","\n","Start of epoch 6\n","Training loss (for one batch) at step 0: 0.0349\n","Training loss (for one batch) at step 31: 0.0324\n","Reconstruction Training acc over epoch: 0.0352\n","KL Divergence: 3.3569509983\n","Validation loss: 0.0352\n","\n","Start of epoch 7\n","Training loss (for one batch) at step 0: 0.0353\n","Training loss (for one batch) at step 31: 0.0362\n","Reconstruction Training acc over epoch: 0.0346\n","KL Divergence: 3.3874907494\n","Validation loss: 0.0337\n","\n","Start of epoch 8\n","Training loss (for one batch) at step 0: 0.0334\n","Training loss (for one batch) at step 31: 0.0342\n","Reconstruction Training acc over epoch: 0.0336\n","KL Divergence: 3.0456216335\n","Validation loss: 0.0342\n","\n","Start of epoch 9\n","Training loss (for one batch) at step 0: 0.0335\n","Training loss (for one batch) at step 31: 0.0329\n","Reconstruction Training acc over epoch: 0.0327\n","KL Divergence: 2.6819970608\n","Validation loss: 0.0322\n","\n","Start of epoch 10\n","Training loss (for one batch) at step 0: 0.0313\n","Training loss (for one batch) at step 31: 0.0304\n","Reconstruction Training acc over epoch: 0.0320\n","KL Divergence: 2.3480172157\n","Validation loss: 0.0315\n","\n","Start of epoch 11\n","Training loss (for one batch) at step 0: 0.0315\n","Training loss (for one batch) at step 31: 0.0326\n","Reconstruction Training acc over epoch: 0.0309\n","KL Divergence: 2.6693613529\n","Validation loss: 0.0306\n","\n","Start of epoch 12\n","Training loss (for one batch) at step 0: 0.0320\n","Training loss (for one batch) at step 31: 0.0306\n","Reconstruction Training acc over epoch: 0.0304\n","KL Divergence: 2.4868128300\n","Validation loss: 0.0314\n","\n","Start of epoch 13\n","Training loss (for one batch) at step 0: 0.0330\n","Training loss (for one batch) at step 31: 0.0304\n","Reconstruction Training acc over epoch: 0.0295\n","KL Divergence: 2.4677426815\n","Validation loss: 0.0296\n","\n","Start of epoch 14\n","Training loss (for one batch) at step 0: 0.0302\n","Training loss (for one batch) at step 31: 0.0296\n","Reconstruction Training acc over epoch: 0.0293\n","KL Divergence: 2.1468513012\n","Validation loss: 0.0294\n","\n","Start of epoch 15\n","Training loss (for one batch) at step 0: 0.0291\n","Training loss (for one batch) at step 31: 0.0279\n","Reconstruction Training acc over epoch: 0.0289\n","KL Divergence: 2.3993632793\n","Validation loss: 0.0283\n","\n","Start of epoch 16\n","Training loss (for one batch) at step 0: 0.0293\n","Training loss (for one batch) at step 31: 0.0264\n","Reconstruction Training acc over epoch: 0.0278\n","KL Divergence: 2.4607694149\n","Validation loss: 0.0274\n","\n","Start of epoch 17\n","Training loss (for one batch) at step 0: 0.0282\n","Training loss (for one batch) at step 31: 0.0277\n","Reconstruction Training acc over epoch: 0.0269\n","KL Divergence: 2.2626156807\n","Validation loss: 0.0266\n","\n","Start of epoch 18\n","Training loss (for one batch) at step 0: 0.0249\n","Training loss (for one batch) at step 31: 0.0284\n","Reconstruction Training acc over epoch: 0.0261\n","KL Divergence: 2.1562044621\n","Validation loss: 0.0262\n","\n","Start of epoch 19\n","Training loss (for one batch) at step 0: 0.0269\n","Training loss (for one batch) at step 31: 0.0253\n","Reconstruction Training acc over epoch: 0.0257\n","KL Divergence: 2.0409240723\n","Validation loss: 0.0253\n","\n","Start of epoch 20\n","Training loss (for one batch) at step 0: 0.0251\n","Training loss (for one batch) at step 31: 0.0239\n","Reconstruction Training acc over epoch: 0.0249\n","KL Divergence: 1.9735745192\n","Validation loss: 0.0248\n","\n","Start of epoch 21\n","Training loss (for one batch) at step 0: 0.0242\n","Training loss (for one batch) at step 31: 0.0238\n","Reconstruction Training acc over epoch: 0.0241\n","KL Divergence: 2.0449862480\n","Validation loss: 0.0241\n","\n","Start of epoch 22\n","Training loss (for one batch) at step 0: 0.0243\n","Training loss (for one batch) at step 31: 0.0249\n","Reconstruction Training acc over epoch: 0.0237\n","KL Divergence: 1.6961358786\n","Validation loss: 0.0234\n","\n","Start of epoch 23\n","Training loss (for one batch) at step 0: 0.0237\n","Training loss (for one batch) at step 31: 0.0234\n","Reconstruction Training acc over epoch: 0.0228\n","KL Divergence: 1.7143678665\n","Validation loss: 0.0227\n","\n","Start of epoch 24\n","Training loss (for one batch) at step 0: 0.0224\n","Training loss (for one batch) at step 31: 0.0221\n","Reconstruction Training acc over epoch: 0.0220\n","KL Divergence: 1.6536799669\n","Validation loss: 0.0216\n","\n","Start of epoch 25\n","Training loss (for one batch) at step 0: 0.0211\n","Training loss (for one batch) at step 31: 0.0216\n","Reconstruction Training acc over epoch: 0.0213\n","KL Divergence: 1.6723992825\n","Validation loss: 0.0212\n","\n","Start of epoch 26\n","Training loss (for one batch) at step 0: 0.0208\n","Training loss (for one batch) at step 31: 0.0199\n","Reconstruction Training acc over epoch: 0.0206\n","KL Divergence: 1.6827450991\n","Validation loss: 0.0204\n","\n","Start of epoch 27\n","Training loss (for one batch) at step 0: 0.0200\n","Training loss (for one batch) at step 31: 0.0190\n","Reconstruction Training acc over epoch: 0.0200\n","KL Divergence: 1.5184923410\n","Validation loss: 0.0198\n","\n","Start of epoch 28\n","Training loss (for one batch) at step 0: 0.0189\n","Training loss (for one batch) at step 31: 0.0191\n","Reconstruction Training acc over epoch: 0.0193\n","KL Divergence: 1.4392323494\n","Validation loss: 0.0191\n","\n","Start of epoch 29\n","Training loss (for one batch) at step 0: 0.0185\n","Training loss (for one batch) at step 31: 0.0193\n","Reconstruction Training acc over epoch: 0.0187\n","KL Divergence: 1.3625569344\n","Validation loss: 0.0184\n","\n","Start of epoch 30\n","Training loss (for one batch) at step 0: 0.0187\n","Training loss (for one batch) at step 31: 0.0178\n","Reconstruction Training acc over epoch: 0.0182\n","KL Divergence: 1.3691124916\n","Validation loss: 0.0179\n","\n","Start of epoch 31\n","Training loss (for one batch) at step 0: 0.0179\n","Training loss (for one batch) at step 31: 0.0173\n","Reconstruction Training acc over epoch: 0.0176\n","KL Divergence: 1.2787846327\n","Validation loss: 0.0176\n","\n","Start of epoch 32\n","Training loss (for one batch) at step 0: 0.0177\n","Training loss (for one batch) at step 31: 0.0170\n","Reconstruction Training acc over epoch: 0.0173\n","KL Divergence: 1.3426527977\n","Validation loss: 0.0169\n","\n","Start of epoch 33\n","Training loss (for one batch) at step 0: 0.0170\n","Training loss (for one batch) at step 31: 0.0165\n","Reconstruction Training acc over epoch: 0.0167\n","KL Divergence: 1.3559837341\n","Validation loss: 0.0165\n","\n","Start of epoch 34\n","Training loss (for one batch) at step 0: 0.0160\n","Training loss (for one batch) at step 31: 0.0158\n","Reconstruction Training acc over epoch: 0.0188\n","KL Divergence: 1.1600573063\n","Validation loss: 0.0165\n","\n","Start of epoch 35\n","Training loss (for one batch) at step 0: 0.0167\n","Training loss (for one batch) at step 31: 0.0159\n","Reconstruction Training acc over epoch: 0.0160\n","KL Divergence: 1.0674039125\n","Validation loss: 0.0163\n","\n","Start of epoch 36\n","Training loss (for one batch) at step 0: 0.0160\n","Training loss (for one batch) at step 31: 0.0147\n","Reconstruction Training acc over epoch: 0.0156\n","KL Divergence: 1.1017712355\n","Validation loss: 0.0158\n","\n","Start of epoch 37\n","Training loss (for one batch) at step 0: 0.0155\n","Training loss (for one batch) at step 31: 0.0148\n","Reconstruction Training acc over epoch: 0.0153\n","KL Divergence: 1.0953909159\n","Validation loss: 0.0154\n","\n","Start of epoch 38\n","Training loss (for one batch) at step 0: 0.0154\n","Training loss (for one batch) at step 31: 0.0149\n","Reconstruction Training acc over epoch: 0.0151\n","KL Divergence: 1.1286833286\n","Validation loss: 0.0149\n","\n","Start of epoch 39\n","Training loss (for one batch) at step 0: 0.0148\n","Training loss (for one batch) at step 31: 0.0148\n","Reconstruction Training acc over epoch: 0.0149\n","KL Divergence: 1.1739115715\n","Validation loss: 0.0148\n","\n","Start of epoch 40\n","Training loss (for one batch) at step 0: 0.0150\n","Training loss (for one batch) at step 31: 0.0145\n","Reconstruction Training acc over epoch: 0.0147\n","KL Divergence: 1.0509943962\n","Validation loss: 0.0147\n","\n","Start of epoch 41\n","Training loss (for one batch) at step 0: 0.0147\n","Training loss (for one batch) at step 31: 0.0156\n","Reconstruction Training acc over epoch: 0.0146\n","KL Divergence: 1.1137888432\n","Validation loss: 0.0143\n","\n","Start of epoch 42\n","Training loss (for one batch) at step 0: 0.0142\n","Training loss (for one batch) at step 31: 0.0144\n","Reconstruction Training acc over epoch: 0.0143\n","KL Divergence: 0.9835529327\n","Validation loss: 0.0148\n","\n","Start of epoch 43\n","Training loss (for one batch) at step 0: 0.0140\n","Training loss (for one batch) at step 31: 0.0139\n","Reconstruction Training acc over epoch: 0.0142\n","KL Divergence: 0.9836838245\n","Validation loss: 0.0142\n","\n","Start of epoch 44\n","Training loss (for one batch) at step 0: 0.0143\n","Training loss (for one batch) at step 31: 0.0141\n","Reconstruction Training acc over epoch: 0.0140\n","KL Divergence: 0.9984534979\n","Validation loss: 0.0139\n","\n","Start of epoch 45\n","Training loss (for one batch) at step 0: 0.0138\n","Training loss (for one batch) at step 31: 0.0144\n","Reconstruction Training acc over epoch: 0.0139\n","KL Divergence: 0.9797953367\n","Validation loss: 0.0139\n","\n","Start of epoch 46\n","Training loss (for one batch) at step 0: 0.0137\n","Training loss (for one batch) at step 31: 0.0140\n","Reconstruction Training acc over epoch: 0.0138\n","KL Divergence: 0.8989742398\n","Validation loss: 0.0137\n","\n","Start of epoch 47\n","Training loss (for one batch) at step 0: 0.0135\n","Training loss (for one batch) at step 31: 0.0129\n","Reconstruction Training acc over epoch: 0.0135\n","KL Divergence: 0.8795453906\n","Validation loss: 0.0136\n","\n","Start of epoch 48\n","Training loss (for one batch) at step 0: 0.0133\n","Training loss (for one batch) at step 31: 0.0135\n","Reconstruction Training acc over epoch: 0.0134\n","KL Divergence: 0.8302267194\n","Validation loss: 0.0134\n","\n","Start of epoch 49\n","Training loss (for one batch) at step 0: 0.0131\n","Training loss (for one batch) at step 31: 0.0130\n","Reconstruction Training acc over epoch: 0.0132\n","KL Divergence: 0.8162313700\n","Validation loss: 0.0132\n","\n","Start of epoch 50\n","Training loss (for one batch) at step 0: 0.0132\n","Training loss (for one batch) at step 31: 0.0124\n","Reconstruction Training acc over epoch: 0.0130\n","KL Divergence: 0.7582433224\n","Validation loss: 0.0131\n","\n","Start of epoch 51\n","Training loss (for one batch) at step 0: 0.0132\n","Training loss (for one batch) at step 31: 0.0135\n","Reconstruction Training acc over epoch: 0.0129\n","KL Divergence: 0.7871406674\n","Validation loss: 0.0128\n","\n","Start of epoch 52\n","Training loss (for one batch) at step 0: 0.0131\n","Training loss (for one batch) at step 31: 0.0121\n","Reconstruction Training acc over epoch: 0.0127\n","KL Divergence: 0.6971715689\n","Validation loss: 0.0127\n","\n","Start of epoch 53\n","Training loss (for one batch) at step 0: 0.0125\n","Training loss (for one batch) at step 31: 0.0127\n","Reconstruction Training acc over epoch: 0.0125\n","KL Divergence: 0.7026228309\n","Validation loss: 0.0122\n","\n","Start of epoch 54\n","Training loss (for one batch) at step 0: 0.0125\n","Training loss (for one batch) at step 31: 0.0120\n","Reconstruction Training acc over epoch: 0.0122\n","KL Divergence: 0.7122437954\n","Validation loss: 0.0121\n","\n","Start of epoch 55\n","Training loss (for one batch) at step 0: 0.0121\n","Training loss (for one batch) at step 31: 0.0115\n","Reconstruction Training acc over epoch: 0.0120\n","KL Divergence: 0.5602251291\n","Validation loss: 0.0120\n","\n","Start of epoch 56\n","Training loss (for one batch) at step 0: 0.0120\n","Training loss (for one batch) at step 31: 0.0119\n","Reconstruction Training acc over epoch: 0.0117\n","KL Divergence: 0.5499218702\n","Validation loss: 0.0116\n","\n","Start of epoch 57\n","Training loss (for one batch) at step 0: 0.0114\n","Training loss (for one batch) at step 31: 0.0114\n","Reconstruction Training acc over epoch: 0.0114\n","KL Divergence: 0.4807693064\n","Validation loss: 0.0113\n","\n","Start of epoch 58\n","Training loss (for one batch) at step 0: 0.0110\n","Training loss (for one batch) at step 31: 0.0108\n","Reconstruction Training acc over epoch: 0.0110\n","KL Divergence: 0.3967930079\n","Validation loss: 0.0109\n","\n","Start of epoch 59\n","Training loss (for one batch) at step 0: 0.0107\n","Training loss (for one batch) at step 31: 0.0115\n","Reconstruction Training acc over epoch: 0.0107\n","KL Divergence: 0.3506886363\n","Validation loss: 0.0105\n","\n","Start of epoch 60\n","Training loss (for one batch) at step 0: 0.0101\n","Training loss (for one batch) at step 31: 0.0092\n","Reconstruction Training acc over epoch: 0.0102\n","KL Divergence: 0.2634882331\n","Validation loss: 0.0100\n","\n","Start of epoch 61\n","Training loss (for one batch) at step 0: 0.0098\n","Training loss (for one batch) at step 31: 0.0094\n","Reconstruction Training acc over epoch: 0.0098\n","KL Divergence: 0.2274393141\n","Validation loss: 0.0094\n","\n","Start of epoch 62\n","Training loss (for one batch) at step 0: 0.0096\n","Training loss (for one batch) at step 31: 0.0086\n","Reconstruction Training acc over epoch: 0.0091\n","KL Divergence: 0.1427169144\n","Validation loss: 0.0090\n","\n","Start of epoch 63\n","Training loss (for one batch) at step 0: 0.0090\n","Training loss (for one batch) at step 31: 0.0083\n","Reconstruction Training acc over epoch: 0.0086\n","KL Divergence: 0.1043822244\n","Validation loss: 0.0083\n","\n","Start of epoch 64\n","Training loss (for one batch) at step 0: 0.0082\n","Training loss (for one batch) at step 31: 0.0079\n","Reconstruction Training acc over epoch: 0.0081\n","KL Divergence: 0.0416775160\n","Validation loss: 0.0078\n","\n","Start of epoch 65\n","Training loss (for one batch) at step 0: 0.0077\n","Training loss (for one batch) at step 31: 0.0072\n","Reconstruction Training acc over epoch: 0.0075\n","KL Divergence: 0.0169807486\n","Validation loss: 0.0073\n","\n","Start of epoch 66\n","Training loss (for one batch) at step 0: 0.0072\n","Training loss (for one batch) at step 31: 0.0071\n","Reconstruction Training acc over epoch: 0.0072\n","KL Divergence: 0.0122786975\n","Validation loss: 0.0070\n","\n","Start of epoch 67\n","Training loss (for one batch) at step 0: 0.0071\n","Training loss (for one batch) at step 31: 0.0072\n","Reconstruction Training acc over epoch: 0.0070\n","KL Divergence: 0.0091823125\n","Validation loss: 0.0069\n","\n","Start of epoch 68\n","Training loss (for one batch) at step 0: 0.0069\n","Training loss (for one batch) at step 31: 0.0068\n","Reconstruction Training acc over epoch: 0.0069\n","KL Divergence: 0.0066294493\n","Validation loss: 0.0069\n","\n","Start of epoch 69\n","Training loss (for one batch) at step 0: 0.0069\n","Training loss (for one batch) at step 31: 0.0068\n","Reconstruction Training acc over epoch: 0.0069\n","KL Divergence: 0.0047491435\n","Validation loss: 0.0069\n","\n","Start of epoch 70\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0070\n","Reconstruction Training acc over epoch: 0.0069\n","KL Divergence: 0.0033948568\n","Validation loss: 0.0068\n","\n","Start of epoch 71\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0068\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0024436207\n","Validation loss: 0.0068\n","\n","Start of epoch 72\n","Training loss (for one batch) at step 0: 0.0069\n","Training loss (for one batch) at step 31: 0.0066\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0018706662\n","Validation loss: 0.0068\n","\n","Start of epoch 73\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0068\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0013011120\n","Validation loss: 0.0068\n","\n","Start of epoch 74\n","Training loss (for one batch) at step 0: 0.0069\n","Training loss (for one batch) at step 31: 0.0067\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0009682387\n","Validation loss: 0.0068\n","\n","Start of epoch 75\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0068\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0007177399\n","Validation loss: 0.0068\n","\n","Start of epoch 76\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0069\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0005388281\n","Validation loss: 0.0068\n","\n","Start of epoch 77\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0069\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0004066769\n","Validation loss: 0.0068\n","\n","Start of epoch 78\n","Training loss (for one batch) at step 0: 0.0069\n","Training loss (for one batch) at step 31: 0.0068\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0003082646\n","Validation loss: 0.0068\n","\n","Start of epoch 79\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0068\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0002341673\n","Validation loss: 0.0068\n","\n","Start of epoch 80\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0067\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0001816544\n","Validation loss: 0.0068\n","\n","Start of epoch 81\n","Training loss (for one batch) at step 0: 0.0069\n","Training loss (for one batch) at step 31: 0.0069\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0001371039\n","Validation loss: 0.0068\n","\n","Start of epoch 82\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0069\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0001144102\n","Validation loss: 0.0068\n","\n","Start of epoch 83\n","Training loss (for one batch) at step 0: 0.0069\n","Training loss (for one batch) at step 31: 0.0068\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0000791653\n","Validation loss: 0.0068\n","\n","Start of epoch 84\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0067\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0000638624\n","Validation loss: 0.0068\n","\n","Start of epoch 85\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0070\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0000461534\n","Validation loss: 0.0068\n","\n","Start of epoch 86\n","Training loss (for one batch) at step 0: 0.0067\n","Training loss (for one batch) at step 31: 0.0067\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0000352665\n","Validation loss: 0.0068\n","\n","Start of epoch 87\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0068\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0000271108\n","Validation loss: 0.0068\n","\n","Start of epoch 88\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0068\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0000208750\n","Validation loss: 0.0068\n","\n","Start of epoch 89\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0067\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0000161492\n","Validation loss: 0.0068\n","\n","Start of epoch 90\n","Training loss (for one batch) at step 0: 0.0069\n","Training loss (for one batch) at step 31: 0.0069\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0000125949\n","Validation loss: 0.0068\n","\n","Start of epoch 91\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0068\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0000143744\n","Validation loss: 0.0068\n","\n","Start of epoch 92\n","Training loss (for one batch) at step 0: 0.0070\n","Training loss (for one batch) at step 31: 0.0069\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0000078656\n","Validation loss: 0.0068\n","\n","Start of epoch 93\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0068\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0000061492\n","Validation loss: 0.0068\n","\n","Start of epoch 94\n","Training loss (for one batch) at step 0: 0.0069\n","Training loss (for one batch) at step 31: 0.0069\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0000049808\n","Validation loss: 0.0068\n","\n","Start of epoch 95\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0066\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0000043471\n","Validation loss: 0.0068\n","\n","Start of epoch 96\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0070\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0000980423\n","Validation loss: 0.0068\n","\n","Start of epoch 97\n","Training loss (for one batch) at step 0: 0.0068\n","Training loss (for one batch) at step 31: 0.0069\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0000029237\n","Validation loss: 0.0068\n","\n","Start of epoch 98\n","Training loss (for one batch) at step 0: 0.0067\n","Training loss (for one batch) at step 31: 0.0068\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0000554802\n","Validation loss: 0.0068\n","\n","Start of epoch 99\n","Training loss (for one batch) at step 0: 0.0069\n","Training loss (for one batch) at step 31: 0.0068\n","Reconstruction Training acc over epoch: 0.0068\n","KL Divergence: 0.0000019688\n","Validation loss: 0.0068\n"]}]},{"cell_type":"code","source":["predictions = vae.predict(X_test_vals, X_test_mask)"],"metadata":{"id":"m9nGo9vLRo6Y","executionInfo":{"status":"ok","timestamp":1647235910216,"user_tz":420,"elapsed":3086,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}}},"id":"m9nGo9vLRo6Y","execution_count":22,"outputs":[]},{"cell_type":"code","source":["predictions = None"],"metadata":{"id":"lk4rBUre60iL","executionInfo":{"status":"ok","timestamp":1647236045806,"user_tz":420,"elapsed":336,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}}},"id":"lk4rBUre60iL","execution_count":29,"outputs":[]},{"cell_type":"code","source":["predictions.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FvvkcSTJfACW","executionInfo":{"status":"ok","timestamp":1647235912558,"user_tz":420,"elapsed":313,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}},"outputId":"1095ed16-d8db-4c93-beec-6df2e456ec3b"},"id":"FvvkcSTJfACW","execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3942, 44772)"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["len(markers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nr7FPIXkgac4","executionInfo":{"status":"ok","timestamp":1647235914707,"user_tz":420,"elapsed":411,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}},"outputId":"0d9290e8-5d2f-4be0-8cf3-91e5b0f0b2c2"},"id":"Nr7FPIXkgac4","execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["44772"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["temp = pd.DataFrame(predictions, columns=markers, index=y_test.index.values)\n","temp['L3'] = y_test.values\n","print(temp.shape)\n","\n","temp.to_pickle('/content/drive/My Drive/methyl_impute/test_predictions_all_genes.pickle')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KXxBEvife1xB","executionInfo":{"status":"ok","timestamp":1647235966961,"user_tz":420,"elapsed":3902,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}},"outputId":"ee6cca5c-0641-4f53-a25a-46c9a48114c0"},"id":"KXxBEvife1xB","execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["(3942, 44773)\n"]}]},{"cell_type":"code","source":["temp = None"],"metadata":{"id":"ayf2hkzy6tq8","executionInfo":{"status":"ok","timestamp":1647236022767,"user_tz":420,"elapsed":467,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}}},"id":"ayf2hkzy6tq8","execution_count":28,"outputs":[]},{"cell_type":"code","execution_count":31,"id":"f4855304","metadata":{"id":"f4855304","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647236128666,"user_tz":420,"elapsed":5956,"user":{"displayName":"EMILY MACIEJEWSKI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06440088281541473321"}},"outputId":"2709b5ec-ab2b-4e1d-a528-a8d925db7da8"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(0.006814523288417324, shape=(), dtype=float64)\n","0.010413017228384632\n"]}],"source":["# vae.correlation_accuracy(X_train)\n","# vae.correlation_accuracy(X_test_vals, X_test_mask)\n","# vae.mse_error(X_train)\n","vae.mse_error(X_test_vals, X_test_mask)"]}],"metadata":{"kernelspec":{"display_name":"Python [conda env:tf-gpu] *","language":"python","name":"conda-env-tf-gpu-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"name":"VAE_impute (variational) (custom dropout+custom loss).ipynb","provenance":[{"file_id":"1l6i2MRYlQfDWqHskKLrrZwZwcj_GSUpQ","timestamp":1647208360086},{"file_id":"1UKfEK6MV2zMjFjIa-uCFK4ph_gTL3go-","timestamp":1646172568867}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}